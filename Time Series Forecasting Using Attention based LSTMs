# Advanced Time Series Forecasting with Attention-based LSTMs

# IMPORT LIBRARIES

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error

from statsmodels.tsa.statespace.sarimax import SARIMAX

import tensorflow as tf
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import (
    Input, LSTM, Dense, Dropout, Layer
)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

import warnings
warnings.filterwarnings("ignore")

np.random.seed(42)
tf.random.set_seed(42)

# LOAD DATA

url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/household_power_consumption.txt"

df = pd.read_csv(
    url,
    sep=';',
    low_memory=False,
    parse_dates={'datetime': ['Date', 'Time']},
    infer_datetime_format=True,
    na_values=['?']
)

df.set_index('datetime', inplace=True)
df = df.astype(float)

# Use subset to reduce memory
df = df.resample('H').mean()

print(df.head())

# PREPROCESSING

# Fill missing values
df.interpolate(method='time', inplace=True)

# Temporal features
df['hour'] = df.index.hour
df['dayofweek'] = df.index.dayofweek
df['month'] = df.index.month

target = 'Global_active_power'
features = df.columns.tolist()

scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(df)

scaled_df = pd.DataFrame(scaled_data, columns=features, index=df.index)


# CREATE SEQUENCES

def create_sequences(data, target_col, window=24):
    X, y = [], []
    target_idx = data.columns.get_loc(target_col)
    
    for i in range(window, len(data)):
        X.append(data.iloc[i-window:i].values)
        y.append(data.iloc[i, target_idx])
    
    return np.array(X), np.array(y)

WINDOW_SIZE = 24

X, y = create_sequences(scaled_df, target)

split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]


# CUSTOM ATTENTION LAYER

class BahdanauAttention(Layer):
    def __init__(self, units):
        super().__init__()
        self.W1 = Dense(units)
        self.W2 = Dense(units)
        self.V = Dense(1)

    def call(self, values):
        score = self.V(tf.nn.tanh(self.W1(values)))
        attention_weights = tf.nn.softmax(score, axis=1)
        context_vector = attention_weights * values
        context_vector = tf.reduce_sum(context_vector, axis=1)
        return context_vector, attention_weights

# ATTENTION LSTM MODEL

def build_attention_lstm(input_shape, lr=0.001):
    inputs = Input(shape=input_shape)
    
    lstm_out = LSTM(64, return_sequences=True)(inputs)
    lstm_out = Dropout(0.2)(lstm_out)
    
    context_vector, attention_weights = BahdanauAttention(32)(lstm_out)
    
    output = Dense(1)(context_vector)
    
    model = Model(inputs, output)
    model.compile(
        optimizer=Adam(lr),
        loss='mse'
    )
    return model

att_model = build_attention_lstm(X_train.shape[1:])
att_model.summary()


# TRAIN MODEL

early_stop = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

history = att_model.fit(
    X_train, y_train,
    validation_split=0.2,
    epochs=30,
    batch_size=64,
    callbacks=[early_stop],
    verbose=1
)


# STANDARD LSTM

lstm_model = Sequential([
    LSTM(64, input_shape=X_train.shape[1:]),
    Dense(1)
])

lstm_model.compile(optimizer='adam', loss='mse')

lstm_model.fit(
    X_train, y_train,
    epochs=30,
    batch_size=64,
    validation_split=0.2,
    callbacks=[early_stop],
    verbose=1
)


# SARIMA MODEL

sarima_data = df[target].iloc[:split + WINDOW_SIZE]

sarima_model = SARIMAX(
    sarima_data,
    order=(1,1,1),
    seasonal_order=(1,1,1,24)
).fit(disp=False)

sarima_forecast = sarima_model.forecast(len(y_test))


# EVALUATION METRICS

def evaluate(true, pred):
    rmse = np.sqrt(mean_squared_error(true, pred))
    mae = mean_absolute_error(true, pred)
    mape = np.mean(np.abs((true - pred) / true)) * 100
    return rmse, mae, mape

# Predictions
att_pred = att_model.predict(X_test).flatten()
lstm_pred = lstm_model.predict(X_test).flatten()

# Inverse scaling
target_idx = features.index(target)

def inverse_transform(pred):
    temp = np.zeros((len(pred), len(features)))
    temp[:, target_idx] = pred
    return scaler.inverse_transform(temp)[:, target_idx]

y_true = inverse_transform(y_test)
att_pred = inverse_transform(att_pred)
lstm_pred = inverse_transform(lstm_pred)

# Metrics
print("Attention LSTM:", evaluate(y_true, att_pred))
print("Standard LSTM :", evaluate(y_true, lstm_pred))
print("SARIMA       :", evaluate(y_true[:len(sarima_forecast)], sarima_forecast))


# PLOTS

plt.figure(figsize=(14,6))
plt.plot(y_true[:500], label='Actual')
plt.plot(att_pred[:500], label='Attention LSTM')
plt.plot(lstm_pred[:500], label='Standard LSTM')
plt.legend()
plt.title("Forecast Comparison")
plt.show()


# ATTENTION ANALYSIS

sample = X_test[:1]
_, attention_weights = att_model.layers[2](sample)

plt.figure(figsize=(10,4))
plt.plot(attention_weights.numpy().flatten())
plt.title("Attention Weights Over Time Steps")
plt.xlabel("Time Step")
plt.ylabel("Weight")
plt.show()
